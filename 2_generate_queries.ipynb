{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Generate queries\n",
    "\n",
    "From Step 1, we now have a set of EN/FR pairs that we can use for IR and CLIR evaluations.\n",
    "\n",
    "But first, we need queries. Particularly, we want natural language queries since this is the intended user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('laws_pairs.csv.zip').fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['section_id', 'doc_id', 'type', 'doc_title_eng', 'doc_title_fra',\n",
       "       'section_str_eng', 'section_str_fra', 'heading_str_eng',\n",
       "       'heading_str_fra', 'text_eng', 'text_fra', 'char_cnt_eng',\n",
       "       'char_cnt_fra', 'token_cnt_eng', 'token_cnt_fra'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_combined_eng</th>\n",
       "      <th>text_combined_fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32470</th>\n",
       "      <td>Telecommunications Act\\n &gt; Investigation and E...</td>\n",
       "      <td>Loi sur les télécommunications\\n &gt; Enquêtes et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58177</th>\n",
       "      <td>Canada Occupational Health and Safety Regulati...</td>\n",
       "      <td>Règlement canadien sur la santé et la sécurité...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13221</th>\n",
       "      <td>Excise Tax Act\\n &gt; Air Transportation Tax &gt; Pe...</td>\n",
       "      <td>Loi sur la taxe d’accise\\n &gt; Taxe de transport...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50300</th>\n",
       "      <td>Apprentice Loans Regulations\\n &gt; Removal of Re...</td>\n",
       "      <td>Règlement sur les prêts aux apprentis\\n &gt; Levé...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28950</th>\n",
       "      <td>Pension Act\\n &gt; Pensions &gt; Pensions for Death\\...</td>\n",
       "      <td>Loi sur les pensions\\n &gt; Pensions &gt; Pensions p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text_combined_eng  \\\n",
       "32470  Telecommunications Act\\n > Investigation and E...   \n",
       "58177  Canada Occupational Health and Safety Regulati...   \n",
       "13221  Excise Tax Act\\n > Air Transportation Tax > Pe...   \n",
       "50300  Apprentice Loans Regulations\\n > Removal of Re...   \n",
       "28950  Pension Act\\n > Pensions > Pensions for Death\\...   \n",
       "\n",
       "                                       text_combined_fra  \n",
       "32470  Loi sur les télécommunications\\n > Enquêtes et...  \n",
       "58177  Règlement canadien sur la santé et la sécurité...  \n",
       "13221  Loi sur la taxe d’accise\\n > Taxe de transport...  \n",
       "50300  Règlement sur les prêts aux apprentis\\n > Levé...  \n",
       "28950  Loi sur les pensions\\n > Pensions > Pensions p...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combine_text(row):\n",
    "    nl = \"\\n\"\n",
    "    return (\n",
    "        f\"{row['doc_title_eng']}\\n\"\n",
    "        f\"{' > ' + row['heading_str_eng'] + nl if row['heading_str_eng'] else ''}\"\n",
    "        f\"{row['section_str_eng']}\\n\"\n",
    "        f\"---\\n{row['text_eng']}\"\n",
    "    ), (\n",
    "        f\"{row['doc_title_fra']}\\n\"\n",
    "        f\"{' > ' + row['heading_str_fra'] + nl if row['heading_str_fra'] else ''}\"\n",
    "        f\"{row['section_str_fra']}\\n\"\n",
    "        f\"---\\n{row['text_fra']}\"\n",
    "    )\n",
    "\n",
    "combined_texts = df.apply(combine_text, axis=1)\n",
    "df[\"text_combined_eng\"] = [x[0] for x in combined_texts]\n",
    "df[\"text_combined_fra\"] = [x[1] for x in combined_texts]\n",
    "\n",
    "df[['text_combined_eng', 'text_combined_fra']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "small_df = df.sample(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question generation with LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap as tr\n",
    "\n",
    "import nest_asyncio\n",
    "from azure.identity import DefaultAzureCredential, ManagedIdentityCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import ServiceContext, set_global_service_context\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.core.prompts import ChatMessage, ChatPromptTemplate, MessageRole\n",
    "from tqdm import tqdm\n",
    "\n",
    "# This is a hack to get some things to work in Jupyter Notebooks\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def pwrap(text):\n",
    "    print(tr.fill(str(text), width=80))\n",
    "\n",
    "# Load environment variables from .env file\n",
    "try:\n",
    "    load_dotenv(dotenv_path=\".env\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# If we're running on Azure, use the Managed Identity to get the secrets\n",
    "if os.environ.get(\"CREDENTIAL_TYPE\").lower() == \"managed\":\n",
    "    credential = ManagedIdentityCredential()\n",
    "else:\n",
    "    credential = DefaultAzureCredential()\n",
    "\n",
    "# Login to KeyVault using Azure credentials\n",
    "client = SecretClient(\n",
    "    vault_url=os.environ.get(\"AZURE_KEYVAULT_URL\"), credential=credential\n",
    ")\n",
    "\n",
    "OPENAI_API_BASE = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "OPENAI_API_VERSION = os.environ.get(\"AZURE_OPENAI_VERSION\")\n",
    "OPENAI_API_KEY = client.get_secret(\"OPENAI-SERVICE-KEY\").value\n",
    "\n",
    "api_key = OPENAI_API_KEY\n",
    "azure_endpoint = OPENAI_API_BASE\n",
    "api_version = OPENAI_API_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate easy questions with GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "QUESTION_GEN_USER_TMPL = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"generate the relevant question.\"\n",
    ")\n",
    "\n",
    "QUESTION_GEN_SYS_TMPL = \"\"\"\\\n",
    "You are labelling an cross-language information retrieval (CLIR) dataset.\n",
    "You are given a chunk of context information, which will be in {language}.\n",
    "Generate a question, in Canadian {language}, that relates to the context information.\n",
    "The questions will be used to evaluate the quality of the information retrieval system.\n",
    "Restrict the question to the context information provided.\\\n",
    "\"\"\"\n",
    "\n",
    "question_gen_template = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage(role=MessageRole.SYSTEM, content=QUESTION_GEN_SYS_TMPL),\n",
    "        ChatMessage(role=MessageRole.USER, content=QUESTION_GEN_USER_TMPL),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def generate_queries(texts, language=\"english\"):\n",
    "    queries = []\n",
    "    llm = AzureOpenAI(\n",
    "        model=\"gpt-35-turbo\",\n",
    "        deployment_name=\"gpt-35-turbo-unfiltered\",\n",
    "        api_key=api_key,\n",
    "        azure_endpoint=azure_endpoint,\n",
    "        api_version=api_version,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    for i, text in enumerate(tqdm(texts)):\n",
    "        fmt_messages = question_gen_template.format_messages(\n",
    "            context_str=text,\n",
    "            language=language,\n",
    "        )\n",
    "        try:\n",
    "            chat_response = llm.chat(fmt_messages)\n",
    "            queries.append(chat_response.message.content)\n",
    "        except:\n",
    "            queries.append(\"\")\n",
    "\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# The results need to be returned in the same order as the input\n",
    "# so we can use the joblib backend \"loky\" to ensure that\n",
    "easy_eng_queries = Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "    delayed(generate_queries)(small_df[\"text_combined_eng\"][i:i+5], \"English\")\n",
    "    for i in range(0, len(small_df), 5)\n",
    ")\n",
    "\n",
    "easy_eng_queries = [q for sublist in easy_eng_queries for q in sublist]\n",
    "\n",
    "with open(\"easy_queries_eng.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(easy_eng_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jkuehn\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "easy_fra_queries = Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "    delayed(generate_queries)(small_df[\"text_combined_fra\"][i:i+5], \"French\")\n",
    "    for i in range(0, len(small_df), 5)\n",
    ")\n",
    "\n",
    "easy_fra_queries = [q for sublist in easy_fra_queries for q in sublist]\n",
    "\n",
    "with open(\"easy_queries_fra.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(easy_fra_queries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate fuzzier questions with GPT-4\n",
    "\n",
    "The previously generated questions might actually be too easy for a keyword retriever, since they tend to use the exact words.\n",
    "\n",
    "This time, we'll try prompting the model to make fuzzier questions that may have synonyms etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_GEN_SYS_TMPL_HARD = \"\"\"\\\n",
    "You are labelling an cross-language information retrieval (CLIR) dataset.\n",
    "You are given a chunk of context information, which will be in {language}.\n",
    "Generate a question, in Canadian {language}, that relates to the context information.\n",
    "The questions will be used to evaluate the quality of the information retrieval system.\n",
    "\n",
    "Make the question slightly difficult for the IR system!\n",
    "For example, use synonyms or paraphrasing rather than the exact words in the context.\n",
    "You can include misspellings!\n",
    "You may also ask more general questions, which the context only partially answers,\n",
    "but it should still be possible for a very good retriever to find this context chunk\n",
    "when given your question.\n",
    "The questions should be in simple {language} and at most 10 words long.\n",
    "\n",
    "Restrict the question to the context information provided.\\\n",
    "\"\"\"\n",
    "\n",
    "question_gen_template_hard = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage(role=MessageRole.SYSTEM, content=QUESTION_GEN_SYS_TMPL_HARD),\n",
    "        ChatMessage(role=MessageRole.USER, content=QUESTION_GEN_USER_TMPL),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def generate_queries_hard(texts, language=\"English\"):\n",
    "    queries = []\n",
    "    llm = AzureOpenAI(\n",
    "        model=\"gpt-4\",\n",
    "        deployment_name=\"gpt-4-unfiltered\",\n",
    "        api_key=api_key,\n",
    "        azure_endpoint=azure_endpoint,\n",
    "        api_version=api_version,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    for i, text in enumerate(tqdm(texts)):\n",
    "        fmt_messages = question_gen_template_hard.format_messages(\n",
    "            context_str=text,\n",
    "            language=language,\n",
    "        )\n",
    "        try:\n",
    "            chat_response = llm.chat(fmt_messages)\n",
    "            queries.append(chat_response.message.content)\n",
    "        except:\n",
    "            queries.append(\"\")\n",
    "\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_eng_queries = Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "    delayed(generate_queries_hard)(small_df[\"text_combined_eng\"][i:i+5], \"English\")\n",
    "    for i in range(0, len(small_df), 5)\n",
    ")\n",
    "\n",
    "hard_eng_queries = [q for sublist in hard_eng_queries for q in sublist]\n",
    "\n",
    "# Save queries to hard_queries_eng.txt\n",
    "with open(\"hard_queries_eng.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(hard_eng_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_fra_queries = Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "    delayed(generate_queries_hard)(small_df[\"text_combined_fra\"][i:i+5], \"French\")\n",
    "    for i in range(0, len(small_df), 5)\n",
    ")\n",
    "\n",
    "hard_fra_queries = [q for sublist in hard_fra_queries for q in sublist]\n",
    "\n",
    "# Save queries to hard_queries_fra.txt\n",
    "with open(\"hard_queries_fra.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(hard_fra_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df[\"easy_eng_queries\"] = easy_eng_queries\n",
    "small_df[\"easy_fra_queries\"] = easy_fra_queries\n",
    "small_df[\"hard_eng_queries\"] = hard_eng_queries\n",
    "small_df[\"hard_fra_queries\"] = hard_fra_queries\n",
    "\n",
    "small_df.to_csv(\"small_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 missing easy English queries\n",
      "0 missing hard English queries\n",
      "1 missing easy French queries\n",
      "18 missing hard French queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "100%|██████████| 18/18 [00:34<00:00,  1.93s/it]\n"
     ]
    }
   ],
   "source": [
    "# The parallel calls to the Azure API may have failed for some of the queries.\n",
    "# Fill in the missing values by running those queries again (without parallelization)\n",
    "missing_easy_eng = small_df[small_df[\"easy_eng_queries\"] == \"\"]\n",
    "print(len(missing_easy_eng), \"missing easy English queries\")\n",
    "missing_hard_eng = small_df[small_df[\"hard_eng_queries\"] == \"\"]\n",
    "print(len(missing_hard_eng), \"missing hard English queries\")\n",
    "missing_easy_fra = small_df[small_df[\"easy_fra_queries\"] == \"\"]\n",
    "print(len(missing_easy_fra), \"missing easy French queries\")\n",
    "missing_hard_fra = small_df[small_df[\"hard_fra_queries\"] == \"\"]\n",
    "print(len(missing_hard_fra), \"missing hard French queries\")\n",
    "\n",
    "fill_easy_eng = generate_queries(missing_easy_eng[\"text_combined_eng\"], \"English\")\n",
    "fill_hard_eng = generate_queries_hard(missing_hard_eng[\"text_combined_eng\"], \"English\")\n",
    "fill_easy_fra = generate_queries(missing_easy_fra[\"text_combined_fra\"], \"French\")\n",
    "fill_hard_fra = generate_queries_hard(missing_hard_fra[\"text_combined_fra\"], \"French\")\n",
    "\n",
    "small_df.loc[missing_easy_eng.index, \"easy_eng_queries\"] = fill_easy_eng\n",
    "small_df.loc[missing_hard_eng.index, \"hard_eng_queries\"] = fill_hard_eng\n",
    "small_df.loc[missing_easy_fra.index, \"easy_fra_queries\"] = fill_easy_fra\n",
    "small_df.loc[missing_hard_fra.index, \"hard_fra_queries\"] = fill_hard_fra\n",
    "\n",
    "small_df.to_csv(\"small_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
